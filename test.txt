import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Przykładowe dane (zastąp je swoimi danymi)
full_names = ['Spółka XYZ w Pcimiu', 'Inna Firma AB', ...]
shortnames = ['XYZ-PCI', 'IF-AB', ...]

# Tokenizacja i padding
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(full_names + shortnames)
full_names_seq = tokenizer.texts_to_sequences(full_names)
shortnames_seq = tokenizer.texts_to_sequences(shortnames)

full_names_padded = pad_sequences(full_names_seq, padding='post')
shortnames_padded = pad_sequences(shortnames_seq, padding='post')

# Rozmiar słownika i długości sekwencji
vocab_size = len(tokenizer.word_index) + 1
max_len_full_names = max([len(seq) for seq in full_names_seq])
max_len_shortnames = max([len(seq) for seq in shortnames_seq])

# Podział na zestaw treningowy i testowy
X_train, X_test, y_train, y_test = train_test_split(full_names_padded, shortnames_padded, test_size=0.2)

# Budowa modelu seq2seq
# Enkoder
encoder_inputs = Input(shape=(None,))
enc_emb = Embedding(vocab_size, 10)(encoder_inputs)
encoder_lstm = LSTM(50, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
encoder_states = [state_h, state_c]

# Dekoder
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_size, 10)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_lstm = LSTM(50, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
decoder_dense = Dense(vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')

# Przygotowanie danych dekodera
y_train_dec_input = pad_sequences(y_train, maxlen=max_len_shortnames, padding='post', value=0)
y_train_dec_output = pad_sequences(y_train, maxlen=max_len_shortnames, padding='post', value=0)

# Trenowanie modelu
model.fit([X_train, y_train_dec_input], np.expand_dims(y_train_dec_output, -1), batch_size=32, epochs=50, validation_split=0.2)

# Funkcja do generowania skróconych nazw
def generate_shortname(model, full_name):
    full_name_seq = tokenizer.texts_to_sequences([full_name])
    full_name_padded = pad_sequences(full_name_seq, maxlen=max_len_full_names, padding='post')
    state = encoder_model.predict(full_name_padded)
    
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = tokenizer.word_index['start']  # start token

    stop_condition = False
    shortname = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + state)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_word_index[sampled_token_index]
        shortname += sampled_char

        # Exit condition: either hit max length or find stop character.
        if (sampled_char == 'end' or len(shortname) > max_len_shortnames):
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index

        # Update states
        state = [h, c]

    return shortname

# Przykład użycia
print(generate_shortname(model, 'Nowa Firma XYZ'))
