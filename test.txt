from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# Przygotowanie danych (jak wcześniej opisano)

# Tokenizacja
tokenizer = Tokenizer(char_level=True)  # Tokenizacja na poziomie znaków
tokenizer.fit_on_texts(data['nazwa_pełna'])

# Konwersja tekstu na sekwencje
sequences = tokenizer.texts_to_sequences(data['nazwa_pełna'])

# Padding sekwencji
max_seq_length = max([len(seq) for seq in sequences])
sequences_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')

# Budowa modelu
model = Sequential([
    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_seq_length),
    LSTM(128, return_sequences=True),
    LSTM(128),
    Dense(64, activation='relu'),
    Dense(len(tokenizer.word_index) + 1, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Trenowanie modelu
model.fit(X_train_pad, y_train, epochs=10, validation_data=(X_test_pad, y_test))


# Funkcja do generowania nowych nazw
def generate_shortname(seed_text, num_chars=10):
    for _ in range(num_chars):
        tokenized_text = tokenizer.texts_to_sequences([seed_text])[0]
        tokenized_text = pad_sequences([tokenized_text], maxlen=max_seq_length, padding='post')
        predicted_char_index = model.predict_classes(tokenized_text, verbose=0)
        seed_text += tokenizer.index_word[predicted_char_index]
    return seed_text

# Generowanie nowej nazwy
new_shortname = generate_shortname("Początek Nazwy")
